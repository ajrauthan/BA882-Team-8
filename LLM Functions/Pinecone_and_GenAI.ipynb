{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3254c4ae044f49bda921930db467b064": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e385af7408c04b6b8c26d7ce1d6cb0cd",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cc6f3839a3f84d7d9bd1d642ad057927",
            "value": 100
          }
        },
        "e385af7408c04b6b8c26d7ce1d6cb0cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "auto"
          }
        },
        "cc6f3839a3f84d7d9bd1d642ad057927": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "black",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Set-up"
      ],
      "metadata": {
        "id": "KvnaOayu8sMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installs\n",
        "! pip install -q pinecone"
      ],
      "metadata": {
        "id": "jPGGEjPvlzKN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e35287ba-0026-4c33-e71a-e739f419539e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/427.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m427.3/427.3 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/87.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m87.7/87.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports - this will take a few moments\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import duckdb\n",
        "\n",
        "import duckdb\n",
        "import langchain\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "import os\n",
        "import uuid\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "import vertexai\n",
        "from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\n",
        "from vertexai.generative_models import GenerativeModel, ChatSession\n",
        "from vertexai.generative_models import Part\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n"
      ],
      "metadata": {
        "id": "TFxxPzO2l2nX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# easiest path, auth with your BU account that you are using on GCP\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "xeDggi8ulS12"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if you have your service account\n",
        "# first upload to the main folder on the left\n",
        "# right click the json service account file and get the path\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/ba882-team8-fall24-4396508fd779.json'"
      ],
      "metadata": {
        "id": "k0141PxLn1eu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project_id = \"ba882-team8-fall24\"   # <--- your project, mine is btibert-ba882-fall24\n",
        "region_id = \"us-central1\"\n",
        "\n",
        "vertexai.init(project=project_id, location=region_id)\n",
        "\n",
        "model = GenerativeModel(\"gemini-1.5-pro-001\")"
      ],
      "metadata": {
        "id": "gckfPnPUo7g2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets define a prompt\n",
        "prompt = \"\"\"\n",
        "Help me learn about Generative AI on VertexAI with python.\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "e_0SBu9NqLfC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66bfc841-559b-44d1-9cb5-9dcf50e51e8d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Diving into Generative AI on Vertex AI with Python\n",
            "\n",
            "Vertex AI is Google Cloud's unified machine learning platform that provides tools for building, training, and deploying machine learning models, including those powered by Generative AI. Here's a breakdown of key aspects and how to get started with Python:\n",
            "\n",
            "**1. What is Generative AI?**\n",
            "\n",
            "Generative AI refers to a class of machine learning models that can create new data instances, like text, images, audio, code, and more, that resemble the training data.  Examples include:\n",
            "\n",
            "* **Text Generation:** Language models like GPT-3 can generate human-quality text, write stories, translate languages, and summarize documents.\n",
            "* **Image Generation:** Models like DALL-E and Stable Diffusion can generate realistic images from text descriptions (text-to-image).\n",
            "* **Code Generation:**  Models like Codex can generate code in various programming languages from natural language descriptions.\n",
            "\n",
            "**2. Generative AI on Vertex AI**\n",
            "\n",
            "Vertex AI provides a managed environment to leverage powerful generative models through APIs without needing to manage the infrastructure:\n",
            "\n",
            "* **Pre-trained Models:** Access pre-trained Generative AI models like PaLM 2 for text generation, Imagen for image generation, and Codey for code generation. \n",
            "* **Model Garden:** Explore a collection of Google's foundation models and adapt them to your specific needs with custom data.\n",
            "* **Generative AI Studio:** An intuitive interface to experiment with and prototype generative models without writing code.\n",
            "\n",
            "**3. Using Python with Vertex AI for Generative AI**\n",
            "\n",
            "Here's how to interact with Generative AI APIs on Vertex AI using Python and the Google Cloud Python Client Library:\n",
            "\n",
            "**a. Setting Up**\n",
            "\n",
            "* **Enable Vertex AI API:** In your Google Cloud project, ensure the Vertex AI API is enabled.\n",
            "* **Install Libraries:** Install the necessary Python libraries:\n",
            "   ```bash\n",
            "   pip install google-cloud-aiplatform \n",
            "   ```\n",
            "* **Authentication:** Set up authentication using your service account credentials:\n",
            "   ```python\n",
            "   import os\n",
            "   from google.cloud import aiplatform\n",
            "\n",
            "   # Set your Google Cloud project ID\n",
            "   os.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"your-project-id\" \n",
            "\n",
            "   # Initialize Vertex AI\n",
            "   aiplatform.init(project=os.environ[\"GOOGLE_CLOUD_PROJECT\"], location=\"your-region\")\n",
            "   ```\n",
            "\n",
            "**b. Example: Text Generation with PaLM 2**\n",
            "\n",
            "```python\n",
            "from vertexai.language_models import TextGenerationModel\n",
            "\n",
            "# Load the pre-trained PaLM 2 model for text generation\n",
            "model = TextGenerationModel.from_pretrained(\"text-bison@001\") \n",
            "\n",
            "# Generate text based on a prompt\n",
            "response = model.predict(\n",
            "    \"\"\"Write a creative story about a cat who goes on an adventure.\"\"\"\n",
            ")\n",
            "\n",
            "# Print the generated text\n",
            "print(response.text) \n",
            "```\n",
            "\n",
            "**4. Further Exploration**\n",
            "\n",
            "* **Custom Training:** Fine-tune pre-trained models on your data using Vertex AI's training pipelines to improve performance on specific tasks.\n",
            "* **Model Deployment:** Deploy trained models for inference using Vertex AI endpoints to serve predictions in your applications.\n",
            "* **Generative AI Studio:** Explore the visual interface to experiment with different models, parameters, and datasets.\n",
            "\n",
            "**Key Resources:**\n",
            "\n",
            "* **Vertex AI Documentation:** [https://cloud.google.com/vertex-ai/docs](https://cloud.google.com/vertex-ai/docs)\n",
            "* **Generative AI on Vertex AI:** [https://cloud.google.com/vertex-ai/docs/generative-ai/overview](https://cloud.google.com/vertex-ai/docs/generative-ai/overview)\n",
            "* **Google Cloud Python Client Libraries:** [https://cloud.google.com/python/docs/reference](https://cloud.google.com/python/docs/reference)\n",
            "\n",
            "**Remember:** Generative AI is a powerful tool, and it's crucial to use it responsibly and ethically.  Be mindful of potential biases in the generated content and ensure your applications align with ethical guidelines and best practices. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# what do we have for the response\n",
        "response\n"
      ],
      "metadata": {
        "id": "DNTYkN2ksIXj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "081c1b3a-7e95-4ca4-a2ee-254a7726b2bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "candidates {\n",
              "  content {\n",
              "    role: \"model\"\n",
              "    parts {\n",
              "      text: \"## Generative AI on Vertex AI with Python: A Comprehensive Guide\\n\\nGenerative AI on Vertex AI empowers you to build and deploy powerful AI applications using Google\\'s cutting-edge technology. This guide provides a structured approach to mastering this exciting field:\\n\\n**1. Understanding the Basics:**\\n\\n* **What is Generative AI?** It\\'s a branch of AI focused on creating new content, like text, images, audio, etc., rather than just analyzing existing data. \\n* **Vertex AI:** Google Cloud\\'s unified platform for building and deploying machine learning models, including generative models.\\n\\n**2. Key Generative AI Services on Vertex AI:**\\n\\n* **Generative Language Models (PaLM 2):** Foundation for text generation, summarization, question answering, and more. Accessible through Vertex AI PaLM 2 API.\\n* **Imagen:** Text-to-image generation API for creating realistic and imaginative visuals.\\n* **Codey:** Family of code generation models for code completion, generation, and chat.\\n* **Chiron:** Speech generation model for natural-sounding text-to-speech synthesis.\\n\\n**3. Setting Up Your Environment:**\\n\\n* **Google Cloud Project:** Create a project on Google Cloud Platform and enable the Vertex AI API.\\n* **Google Cloud SDK:** Install and configure the SDK on your local machine to interact with your Cloud resources.\\n* **Python and Libraries:** Install Python and essential libraries like `google-cloud-aiplatform` for interacting with Vertex AI services.\\n\\n**4. Exploring Generative AI with Python Code:**\\n\\nHere\\'s a basic example showcasing text generation using the Vertex AI PaLM 2 API:\\n\\n```python\\nfrom vertexai.language_models import TextGenerationModel\\n\\n# Initialize the model\\nmodel = TextGenerationModel.from_pretrained(\\\"text-bison@001\\\")\\n\\n# Define the input prompt\\nprompt = \\\"Write a short story about a time traveler.\\\"\\n\\n# Generate text\\nresponse = model.predict(prompt)\\n\\n# Print the generated text\\nprint(response.text)\\n```\\n\\n**5. Building Custom Generative Applications:**\\n\\n* **Fine-tuning:** Tailor pre-trained models like PaLM 2 on your own data for specific tasks and domains.\\n* **Custom Models:** Train your own generative models using frameworks like TensorFlow or PyTorch and deploy them on Vertex AI for scalability.\\n* **MLOps:** Leverage Vertex AI\\'s MLOps capabilities for efficient model training, deployment, monitoring, and management.\\n\\n**6. Resources and Further Learning:**\\n\\n* **Vertex AI Documentation:** https://cloud.google.com/vertex-ai/docs\\n* **Generative AI on Google Cloud:** https://cloud.google.com/products/generative-ai\\n* **PaLM 2 API Documentation:** https://cloud.google.com/vertex-ai/docs/generative-ai/language/get-started\\n* **Generative AI Learning Path:** https://cloud.google.com/training/generative-ai\\n\\n**Tips for Success:**\\n\\n* Start with pre-trained models and APIs to get familiar with generative AI concepts.\\n* Explore different prompt engineering techniques to guide model output effectively.\\n* Experiment with various model parameters and settings to optimize performance.\\n* Leverage community resources, tutorials, and examples to accelerate your learning.\\n\\nThis guide offers a comprehensive foundation for understanding and utilizing Generative AI on Vertex AI with Python. Remember, continuous exploration and experimentation are key to unlocking the full potential of this transformative technology! \\n\"\n",
              "    }\n",
              "  }\n",
              "  avg_logprobs: -0.28874227218834686\n",
              "  finish_reason: STOP\n",
              "  safety_ratings {\n",
              "    category: HARM_CATEGORY_HATE_SPEECH\n",
              "    probability: NEGLIGIBLE\n",
              "    probability_score: 0.0306396484\n",
              "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
              "    severity_score: 0.0510253906\n",
              "  }\n",
              "  safety_ratings {\n",
              "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
              "    probability: NEGLIGIBLE\n",
              "    probability_score: 0.0439453125\n",
              "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
              "    severity_score: 0.125976562\n",
              "  }\n",
              "  safety_ratings {\n",
              "    category: HARM_CATEGORY_HARASSMENT\n",
              "    probability: NEGLIGIBLE\n",
              "    probability_score: 0.06640625\n",
              "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
              "    severity_score: 0.0717773438\n",
              "  }\n",
              "  safety_ratings {\n",
              "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
              "    probability: NEGLIGIBLE\n",
              "    probability_score: 0.0311279297\n",
              "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
              "    severity_score: 0.0473632812\n",
              "  }\n",
              "}\n",
              "model_version: \"gemini-1.5-pro-001\"\n",
              "usage_metadata {\n",
              "  prompt_token_count: 15\n",
              "  candidates_token_count: 738\n",
              "  total_token_count: 753\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Database creation"
      ],
      "metadata": {
        "id": "fIOFhSot8zqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "# Initialize BigQuery client\n",
        "client = bigquery.Client()\n",
        "\n",
        "# Define your BigQuery table\n",
        "project_id = \"ba882-team8-fall24\"  # Replace with your GCP project ID\n",
        "dataset_id = \"mbta_dataset\"  # Replace with your dataset ID\n",
        "table_id = \"joined_prediction\"      # Replace with your table ID\n",
        "\n",
        "query = f\"\"\"\n",
        "    SELECT *\n",
        "    FROM `{project_id}.{dataset_id}.{table_id}`\n",
        "\"\"\"\n",
        "\n",
        "# Query BigQuery and load the data into a pandas DataFrame\n",
        "query_job = client.query(query)\n",
        "df = query_job.result().to_dataframe()\n",
        "\n",
        "# Initialize DuckDB and create a database\n",
        "db = duckdb.connect(\"mbta.duckdb\")  # Creates a new DuckDB file\n",
        "\n",
        "# Load the DataFrame into DuckDB as a table\n",
        "db.execute(\"CREATE TABLE mbta AS SELECT * FROM df\")\n",
        "\n",
        "# Verify the table exists in DuckDB\n",
        "result = db.execute(\"SHOW TABLES\").fetchall()\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "3254c4ae044f49bda921930db467b064",
            "e385af7408c04b6b8c26d7ce1d6cb0cd",
            "cc6f3839a3f84d7d9bd1d642ad057927"
          ]
        },
        "id": "QMO_g0I_MhuJ",
        "outputId": "07176043-fabd-4859-a4aa-b9712e24bd5f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3254c4ae044f49bda921930db467b064"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('mbta',)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db.sql(\"show tables\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryZXcptVNH23",
        "outputId": "3265fbe9-9101-4f03-b450-0b8bbf771b95"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
              "‚îÇ  name   ‚îÇ\n",
              "‚îÇ varchar ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ mbta    ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mbta = db.sql(\"select * from mbta;\").df()\n",
        "mbta.shape"
      ],
      "metadata": {
        "id": "-UrCb_2i6gos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56e4c97c-6ee3-4f65-d7aa-5791b3fc0bae"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(259199, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mbta.info()"
      ],
      "metadata": {
        "id": "xQ66e13a6qbw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b5789fc-5bcb-4276-b8b5-8d2258e20071"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 259199 entries, 0 to 259198\n",
            "Data columns (total 30 columns):\n",
            " #   Column                 Non-Null Count   Dtype                  \n",
            "---  ------                 --------------   -----                  \n",
            " 0   prediction_id          259199 non-null  object                 \n",
            " 1   schedule_id            259199 non-null  object                 \n",
            " 2   arrival_time_p         259199 non-null  datetime64[us, Etc/UTC]\n",
            " 3   arrival_time_s         259199 non-null  datetime64[us, Etc/UTC]\n",
            " 4   arrival_uncertainty    21827 non-null   float64                \n",
            " 5   departure_time_p       259199 non-null  datetime64[us, Etc/UTC]\n",
            " 6   departure_time_s       259199 non-null  datetime64[us, Etc/UTC]\n",
            " 7   departure_uncertainty  21827 non-null   float64                \n",
            " 8   direction_id_p         259199 non-null  int64                  \n",
            " 9   direction_id_s         259199 non-null  int64                  \n",
            " 10  last_trip              259199 non-null  bool                   \n",
            " 11  revenue                259199 non-null  object                 \n",
            " 12  schedule_relationship  0 non-null       float64                \n",
            " 13  status                 224 non-null     object                 \n",
            " 14  stop_sequence_p        259199 non-null  int64                  \n",
            " 15  stop_sequence_s        259199 non-null  int64                  \n",
            " 16  update_type            21614 non-null   object                 \n",
            " 17  route_id_p             259199 non-null  object                 \n",
            " 18  route_id_s             259199 non-null  object                 \n",
            " 19  stop_id_p              259199 non-null  object                 \n",
            " 20  stop_id_s              259199 non-null  object                 \n",
            " 21  trip_id                259199 non-null  object                 \n",
            " 22  vehicle_id             259199 non-null  object                 \n",
            " 23  drop_off_type          259199 non-null  int64                  \n",
            " 24  pickup_type            259199 non-null  int64                  \n",
            " 25  stop_headsign          316 non-null     object                 \n",
            " 26  timepoint              259199 non-null  bool                   \n",
            " 27  delay                  259199 non-null  int64                  \n",
            " 28  day                    259199 non-null  object                 \n",
            " 29  time_of_the_day        259199 non-null  object                 \n",
            "dtypes: bool(2), datetime64[us, Etc/UTC](4), float64(3), int64(7), object(14)\n",
            "memory usage: 55.9+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pinecone\n"
      ],
      "metadata": {
        "id": "HA8ypbfK70en"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I added the Pinecone API key to the secrets here in colab\n",
        "# toggle on the switch to allow access within this notebook\n",
        "\n",
        "#pinecone_token = userdata.get('PINECONE_BU')\n",
        "\n",
        "pc = Pinecone(api_key=\"pcsk_4kKuo8_KkUeBkRBCXMTksnXBFDVf8ph7ro9nfN8z5N8qU3iLRXK2HNqZMiCAarvFoZiwm4\")"
      ],
      "metadata": {
        "id": "7gSLHC7an1ig"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you likely wont have any, but I already have some pipelines running\n",
        "# as well as for sandboxing ideas\n",
        "\n",
        "pc.list_indexes()"
      ],
      "metadata": {
        "id": "6UCCf4S370nq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69cfbabe-8c1f-411e-b41e-f5050851f5a0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\n",
              "    {\n",
              "        \"name\": \"mbta\",\n",
              "        \"dimension\": 385,\n",
              "        \"metric\": \"cosine\",\n",
              "        \"host\": \"mbta-k167d5c.svc.gcp-us-central1-4a9f.pinecone.io\",\n",
              "        \"spec\": {\n",
              "            \"serverless\": {\n",
              "                \"cloud\": \"gcp\",\n",
              "                \"region\": \"us-central1\"\n",
              "            }\n",
              "        },\n",
              "        \"status\": {\n",
              "            \"ready\": true,\n",
              "            \"state\": \"Ready\"\n",
              "        },\n",
              "        \"deletion_protection\": \"disabled\"\n",
              "    },\n",
              "    {\n",
              "        \"name\": \"ba882-rag\",\n",
              "        \"dimension\": 768,\n",
              "        \"metric\": \"cosine\",\n",
              "        \"host\": \"ba882-rag-k167d5c.svc.aped-4627-b74a.pinecone.io\",\n",
              "        \"spec\": {\n",
              "            \"serverless\": {\n",
              "                \"cloud\": \"aws\",\n",
              "                \"region\": \"us-east-1\"\n",
              "            }\n",
              "        },\n",
              "        \"status\": {\n",
              "            \"ready\": true,\n",
              "            \"state\": \"Ready\"\n",
              "        },\n",
              "        \"deletion_protection\": \"disabled\"\n",
              "    },\n",
              "    {\n",
              "        \"name\": \"sample-movies\",\n",
              "        \"dimension\": 1024,\n",
              "        \"metric\": \"cosine\",\n",
              "        \"host\": \"sample-movies-k167d5c.svc.aped-4627-b74a.pinecone.io\",\n",
              "        \"spec\": {\n",
              "            \"serverless\": {\n",
              "                \"cloud\": \"aws\",\n",
              "                \"region\": \"us-east-1\"\n",
              "            }\n",
              "        },\n",
              "        \"status\": {\n",
              "            \"ready\": true,\n",
              "            \"state\": \"Ready\"\n",
              "        },\n",
              "        \"deletion_protection\": \"disabled\"\n",
              "    }\n",
              "]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize the embedding model\n",
        "sent_trans = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Function to process a batch of rows\n",
        "def process_batch(batch):\n",
        "    return [sent_trans.encode(f\"{row['day']} {row['time_of_the_day']}\") for _, row in batch.iterrows()]\n",
        "\n",
        "# Split data into batches\n",
        "batch_size = 1000  # Adjust batch size as per available resources\n",
        "batches = [mbta[i:i + batch_size] for i in range(0, len(mbta), batch_size)]\n",
        "\n",
        "# Process batches\n",
        "embeddings = []\n",
        "for batch in tqdm(batches):\n",
        "    embeddings.extend(process_batch(batch))\n",
        "\n",
        "mbta['text_embedding'] = embeddings"
      ],
      "metadata": {
        "id": "uI_LhWwbaE1P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00a178eb-e231-446e-ae80-eca17bb06d61"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 260/260 [28:25<00:00,  6.56s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Normalize numeric features (e.g., 'delay')\n",
        "scaler = MinMaxScaler()\n",
        "mbta['normalized_numeric'] = list(scaler.fit_transform(mbta[['delay']]))"
      ],
      "metadata": {
        "id": "Ag95_PHPd8yK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine embeddings (text + numeric)\n",
        "mbta['combined_vector'] = mbta.apply(\n",
        "    lambda row: np.concatenate([row['text_embedding'], row['normalized_numeric']]), axis=1\n",
        ")"
      ],
      "metadata": {
        "id": "BpADtOs5eNtF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a convention is to create the index if pinecone doesn't have it\n",
        "\n",
        "index_name = \"mbta\"\n",
        "\n",
        "dimension = len(mbta['combined_vector'].iloc[0])\n",
        "\n",
        "if not pc.has_index(index_name):\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=dimension,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(\n",
        "            cloud='gcp', # gcp <- not part of free\n",
        "            region='us-central1' # us-central1 <- not part of free\n",
        "        )\n",
        "    )"
      ],
      "metadata": {
        "id": "NYFnNw4AcPv8"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to the index\n",
        "index_name = \"mbta\"\n",
        "\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "BrXotKrCtvcv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the stats\n",
        "index.describe_index_stats()"
      ],
      "metadata": {
        "id": "uOyeXQZT9ED4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51578651-76db-4ff2-cd21-369ef28e3b86"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 385,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {'': {'vector_count': 259199}},\n",
              " 'total_vector_count': 259199}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 1000  # Adjust this based on your system's capabilities\n",
        "\n",
        "# Function to process and upsert a batch\n",
        "def upsert_batch(index, batch):\n",
        "    vectors = [\n",
        "        {\n",
        "            \"id\": str(idx),  # Unique ID\n",
        "            \"values\": row['combined_vector'],  # Vector to upsert\n",
        "            \"metadata\": {\n",
        "                \"day\": row['day'],  # Supported fields only\n",
        "                \"time_of_the_day\": row['time_of_the_day'],  # Include specific fields\n",
        "                \"delay\": row['delay']  # Numeric field\n",
        "            }\n",
        "        }\n",
        "        for idx, row in batch.iterrows()\n",
        "    ]\n",
        "    index.upsert(vectors)\n",
        "\n",
        "# Process data in batches with progress tracking\n",
        "for i in tqdm(range(0, len(mbta), batch_size), desc=\"Upserting vectors\"):\n",
        "    batch = mbta.iloc[i:i + batch_size]\n",
        "    upsert_batch(index, batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JftOFSlL161T",
        "outputId": "6fb6fa96-03ec-48a2-92c5-b0dc24e5f299"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Upserting vectors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 260/260 [11:53<00:00,  2.74s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using vectorized data"
      ],
      "metadata": {
        "id": "pAjUvLbP9FD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the embedding model\n",
        "sent_trans = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "PtWrHnAUFIVF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###User Input"
      ],
      "metadata": {
        "id": "72iRwXzR-4vV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# User's input\n",
        "user_query = \"What delays are typical on Monday night?\"\n",
        "\n",
        "# Convert query to embedding\n",
        "query_vector = sent_trans.encode(user_query).tolist()\n",
        "query_vector.append(0.0)\n",
        "\n",
        "# Query Pinecone\n",
        "response = index.query(\n",
        "    vector=query_vector,\n",
        "    top_k=5,\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "# Extract metadata\n",
        "results = [\n",
        "    {\n",
        "        \"day\": match['metadata']['day'],\n",
        "        \"time_of_the_day\": match['metadata']['time_of_the_day'],\n",
        "        \"delay\": match['metadata']['delay'],\n",
        "        \"score\": match['score']\n",
        "    }\n",
        "    for match in response['matches']\n",
        "]"
      ],
      "metadata": {
        "id": "QWM_1xX2-uLk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LLM Response"
      ],
      "metadata": {
        "id": "AQG_34CD-8GX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Format the context for Gemini\n",
        "context = \"\\n\".join(\n",
        "    [\n",
        "        f\"Day: {entry['day']}, Time: {entry['time_of_the_day']}, Delay: {entry['delay']} minutes.\"\n",
        "        for entry in results\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Generate a prompt for Gemini\n",
        "prompt = f\"\"\"\n",
        "The user asked: {user_query}\n",
        "Here are some relevant data points:\n",
        "{context}\n",
        "\n",
        "Please interpret the data and provide a summary in a conversational tone.\n",
        "\"\"\"\n",
        "\n",
        "# Use Gemini to generate a response\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBBXLXm2FnCf",
        "outputId": "c447d2a2-24ae-4a07-8b8c-b99f2e49ae1a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hmm, this data is a bit strange! \n",
            "\n",
            "It looks like there was a HUGE negative delay on Tuesday evening.  A negative delay means things were actually really early!  It's unlikely that something would be over 20 hours early four times in a row. There might be an error with how this data was recorded. \n",
            "\n",
            "As for Monday night, there's only one data point showing a 16-minute delay.  \n",
            "\n",
            "Overall, we can't really say what delays are typical on Monday nights based on this information. We need more accurate data about Monday nights specifically to draw any conclusions! \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##More Examples"
      ],
      "metadata": {
        "id": "QWmVpuVHHAI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# User's input\n",
        "user_query = \"What are the typical delays on Saturday nights?\"\n",
        "\n",
        "# Convert query to embedding\n",
        "query_vector = sent_trans.encode(user_query).tolist()\n",
        "query_vector.append(0.0)\n",
        "\n",
        "# Query Pinecone\n",
        "response = index.query(\n",
        "    vector=query_vector,\n",
        "    top_k=5,\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "# Extract metadata\n",
        "results = [\n",
        "    {\n",
        "        \"day\": match['metadata']['day'],\n",
        "        \"time_of_the_day\": match['metadata']['time_of_the_day'],\n",
        "        \"delay\": match['metadata']['delay'],\n",
        "        \"score\": match['score']\n",
        "    }\n",
        "    for match in response['matches']\n",
        "]\n",
        "\n",
        "\n",
        "# Format the context for Gemini\n",
        "context = \"\\n\".join(\n",
        "    [\n",
        "        f\"Day: {entry['day']}, Time: {entry['time_of_the_day']}, Delay: {entry['delay']} minutes.\"\n",
        "        for entry in results\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Generate a prompt for Gemini\n",
        "prompt = f\"\"\"\n",
        "The user asked: {user_query}\n",
        "Here are some relevant data points:\n",
        "{context}\n",
        "\n",
        "Please interpret the data and provide a summary in a conversational tone.\n",
        "\"\"\"\n",
        "\n",
        "# Use Gemini to generate a response\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QVMMgnWHBa6",
        "outputId": "d9f3bc31-5693-41ee-cb96-55456869ece6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like things run pretty smoothly on Saturday nights!  Those negative delays actually mean that things are arriving early by about 25-40 minutes on average.  Maybe everyone is just eager to get the weekend started! üéâ \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# User's input\n",
        "user_query = \"Why might delays be longer on Wednesday afternoons?\"\n",
        "\n",
        "# Convert query to embedding\n",
        "query_vector = sent_trans.encode(user_query).tolist()\n",
        "query_vector.append(0.0)\n",
        "\n",
        "# Query Pinecone\n",
        "response = index.query(\n",
        "    vector=query_vector,\n",
        "    top_k=5,\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "# Extract metadata\n",
        "results = [\n",
        "    {\n",
        "        \"day\": match['metadata']['day'],\n",
        "        \"time_of_the_day\": match['metadata']['time_of_the_day'],\n",
        "        \"delay\": match['metadata']['delay'],\n",
        "        \"score\": match['score']\n",
        "    }\n",
        "    for match in response['matches']\n",
        "]\n",
        "\n",
        "\n",
        "# Format the context for Gemini\n",
        "context = \"\\n\".join(\n",
        "    [\n",
        "        f\"Day: {entry['day']}, Time: {entry['time_of_the_day']}, Delay: {entry['delay']} minutes.\"\n",
        "        for entry in results\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Generate a prompt for Gemini\n",
        "prompt = f\"\"\"\n",
        "The user asked: {user_query}\n",
        "Here are some relevant data points:\n",
        "{context}\n",
        "\n",
        "Please interpret the data and provide a summary in a conversational tone.\n",
        "\"\"\"\n",
        "\n",
        "# Use Gemini to generate a response\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiTG8FUSIs_C",
        "outputId": "e5f29765-3a2a-4f1c-97f1-bc0677080c4b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hmm, this data is a little strange! It looks like there are some seriously huge negative delays, especially on Tuesday evening. A negative delay means things are way ahead of schedule.  1377 minutes is over 22 hours! \n",
            "\n",
            "There's not enough information here to say why delays might be longer on Wednesday afternoons.  \n",
            "\n",
            "Here's what we need to figure this out:\n",
            "\n",
            "* **More Data:** We need data from many Wednesdays, not just one, and ideally from other times on Wednesday too. \n",
            "* **Positive Delays:** Negative delays this large are likely errors. We need to look at data that represents actual delays (positive numbers).\n",
            "* **Context:** What are these delays for? Flights, trains, buses? Knowing the context helps us understand potential reasons for delays. \n",
            "\n",
            "Let's get some better data, and then we can try to solve this mystery! \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# User's input\n",
        "user_query = \"What delays should we expect on Saturday nights next week, and how can we minimize them?\"\n",
        "\n",
        "# Convert query to embedding\n",
        "query_vector = sent_trans.encode(user_query).tolist()\n",
        "query_vector.append(0.0)\n",
        "\n",
        "# Query Pinecone\n",
        "response = index.query(\n",
        "    vector=query_vector,\n",
        "    top_k=5,\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "# Extract metadata\n",
        "results = [\n",
        "    {\n",
        "        \"day\": match['metadata']['day'],\n",
        "        \"time_of_the_day\": match['metadata']['time_of_the_day'],\n",
        "        \"delay\": match['metadata']['delay'],\n",
        "        \"score\": match['score']\n",
        "    }\n",
        "    for match in response['matches']\n",
        "]\n",
        "\n",
        "\n",
        "# Format the context for Gemini\n",
        "context = \"\\n\".join(\n",
        "    [\n",
        "        f\"Day: {entry['day']}, Time: {entry['time_of_the_day']}, Delay: {entry['delay']} minutes.\"\n",
        "        for entry in results\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Generate a prompt for Gemini\n",
        "prompt = f\"\"\"\n",
        "The user asked: {user_query}\n",
        "Here are some relevant data points:\n",
        "{context}\n",
        "\n",
        "Please interpret the data and provide a summary in a conversational tone.\n",
        "\"\"\"\n",
        "\n",
        "# Use Gemini to generate a response\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuhlQFNhJBpV",
        "outputId": "6b49f594-4cff-413d-977c-482cf68aadba"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you're actually consistently EARLY on Saturday nights! Those negative delays mean you're arriving ahead of schedule by an average of about 31 minutes. \n",
            "\n",
            "So, good news! No need to worry about minimizing delays. You seem to have Saturday nights under control! üòâ  \n",
            "\n",
            "Do you want to check data for any other night of the week? \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# User's input\n",
        "user_query = \"How do Monday night delays compare to other nights of the week?\"\n",
        "\n",
        "# Convert query to embedding\n",
        "query_vector = sent_trans.encode(user_query).tolist()\n",
        "query_vector.append(0.0)\n",
        "\n",
        "# Query Pinecone\n",
        "response = index.query(\n",
        "    vector=query_vector,\n",
        "    top_k=5,\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "# Extract metadata\n",
        "results = [\n",
        "    {\n",
        "        \"day\": match['metadata']['day'],\n",
        "        \"time_of_the_day\": match['metadata']['time_of_the_day'],\n",
        "        \"delay\": match['metadata']['delay'],\n",
        "        \"score\": match['score']\n",
        "    }\n",
        "    for match in response['matches']\n",
        "]\n",
        "\n",
        "\n",
        "# Format the context for Gemini\n",
        "context = \"\\n\".join(\n",
        "    [\n",
        "        f\"Day: {entry['day']}, Time: {entry['time_of_the_day']}, Delay: {entry['delay']} minutes.\"\n",
        "        for entry in results\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Generate a prompt for Gemini\n",
        "prompt = f\"\"\"\n",
        "The user asked: {user_query}\n",
        "Here are some relevant data points:\n",
        "{context}\n",
        "\n",
        "Please interpret the data and provide a summary in a conversational tone.\n",
        "\"\"\"\n",
        "\n",
        "# Use Gemini to generate a response\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEsV1JO0JORW",
        "outputId": "ac95fea1-92e4-4ab1-e092-7c8fae440096"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like Monday nights are running super smoothly!  ‚úàÔ∏è\n",
            "\n",
            "The data shows an average delay of **-16 minutes** for Monday nights. That means flights are actually taking off and landing **16 minutes earlier** than scheduled. \n",
            "\n",
            "However, we need more information to compare Monday nights to other nights of the week.  To give you a complete picture, I need data on the average delays for Tuesday, Wednesday, Thursday, Friday, Saturday, and Sunday nights too.  \n",
            "\n",
            "Once you give me that info, I can tell you which night has the shortest (or longest!) delays. üòâ \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# User's input\n",
        "user_query = \"What are the typical delays on Monday nights?\"\n",
        "\n",
        "# Convert query to embedding\n",
        "query_vector = sent_trans.encode(user_query).tolist()\n",
        "query_vector.append(0.0)\n",
        "\n",
        "# Query Pinecone\n",
        "response = index.query(\n",
        "    vector=query_vector,\n",
        "    top_k=20,\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "# Extract metadata\n",
        "results = [\n",
        "    {\n",
        "        \"day\": match['metadata']['day'],\n",
        "        \"time_of_the_day\": match['metadata']['time_of_the_day'],\n",
        "        \"delay\": match['metadata']['delay'],\n",
        "        \"score\": match['score']\n",
        "    }\n",
        "    for match in response['matches']\n",
        "]\n",
        "\n",
        "\n",
        "# Format the context for Gemini\n",
        "context = \"\\n\".join(\n",
        "    [\n",
        "        f\"Day: {entry['day']}, Time: {entry['time_of_the_day']}, Delay: {entry['delay']} minutes.\"\n",
        "        for entry in results\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Generate a prompt for Gemini\n",
        "prompt = f\"\"\"\n",
        "The user asked: {user_query}\n",
        "Here are some relevant data points:\n",
        "{context}\n",
        "\n",
        "Please interpret the data based on the user query and provide a summary in a conversational tone.\n",
        "\"\"\"\n",
        "\n",
        "# Use Gemini to generate a response\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmBchqYaJvsy",
        "outputId": "f1977d16-3d64-4376-83dd-41f53740973b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you're interested in delays on Monday nights!  The data shows that Monday nights typically have very short delays, averaging around **4 to 16 minutes early**.  \n",
            "\n",
            "However, there's something strange going on with the Tuesday evening data. Those delays are showing as almost -1400 minutes, which translates to nearly a full day early! That doesn't sound like a typical delay. It's more likely an error in the data or perhaps a cancellation. \n",
            "\n",
            "Let me know if you want to explore delays on other days or times, I'm happy to help! üòä \n",
            "\n"
          ]
        }
      ]
    }
  ]
}